{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shxvqT233GYG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClbSmKr23Hg_"
   },
   "outputs": [],
   "source": [
    "!pip install torchaudio librosa\n",
    "from google.colab import files\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "from groq import Groq\n",
    "from google.colab import userdata\n",
    "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "print(\" Upload your audio file (.wav, .mp3, etc.)\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "audio_path = list(uploaded.keys())[0]\n",
    "\n",
    "class AudioCNNRNN(nn.Module):\n",
    "    def __init__(self, lstm_hidden_size=128, num_classes=2):\n",
    "        super(AudioCNNRNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=lstm_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, c, h, w = x.size()\n",
    "        c_in = x.view(batch_size * seq_len, c, h, w)\n",
    "        features = self.cnn(c_in)\n",
    "        features = features.mean(dim=[2, 3])\n",
    "        features = features.view(batch_size, seq_len, -1)\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def extract_mel_spectrogram(audio_path, sr=16000, n_mels=64):\n",
    "    waveform, sample_rate = librosa.load(audio_path, sr=sr)\n",
    "    mel_spec = librosa.feature.melspectrogram(y=waveform, sr=sr, n_mels=n_mels)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "def slice_spectrogram(mel_spec, slice_size=128, step=64):\n",
    "    slices = []\n",
    "    for start in range(0, mel_spec.shape[1] - slice_size, step):\n",
    "        slice_ = mel_spec[:, start:start + slice_size]\n",
    "        slices.append(slice_)\n",
    "    return slices\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AudioCNNRNN()\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "mel_spec = extract_mel_spectrogram(audio_path)\n",
    "mel_slices = slice_spectrogram(mel_spec, slice_size=128, step=64)\n",
    "\n",
    "if len(mel_slices) == 0:\n",
    "    raise RuntimeError(\"No mel slices generated. Check audio length.\")\n",
    "\n",
    "tensor_slices = [torch.tensor(s).unsqueeze(0) for s in mel_slices]\n",
    "data = torch.stack(tensor_slices)\n",
    "data = data.unsqueeze(0)\n",
    "data = data.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(data)\n",
    "    logits = outputs\n",
    "\n",
    "temperature = 3.0\n",
    "probabilities = torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "ai_probability = probabilities[0][0].item()\n",
    "human_probability = probabilities[0][1].item()\n",
    "\n",
    "diff = abs(ai_probability - human_probability)\n",
    "if diff >= 0.7:\n",
    "    confidence = \"High\"\n",
    "elif diff >= 0.3:\n",
    "    confidence = \"Medium\"\n",
    "else:\n",
    "    confidence = \"Low\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are an AI audio analysis expert.\n",
    "The detector outputs:\n",
    "- AI-generated probability: {ai_probability:.4f}\n",
    "- Human-generated probability: {human_probability:.4f}\n",
    "- Confidence level: {confidence}\n",
    "\n",
    "Give a short, human-readable explanation (1-2 sentences) of why the audio was likely classified as {'AI-generated' if ai_probability > human_probability else 'human-generated'}.\n",
    "Base it on audio cues such as tone, pitch patterns, unnatural pauses, synthesis artifacts, or other hints you might infer.\n",
    "Avoid repeating probabilities; focus on the reasoning.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.6,\n",
    ")\n",
    "explanation = response.choices[0].message.content.strip()\n",
    "print(f\" AI-generated Probability: {ai_probability:.4f}\")\n",
    "print(f\" Confidence Level: {confidence}\")\n",
    "print(f\" Explanation: {explanation}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMWKlfvv2IwaveEU/hrz29z",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
