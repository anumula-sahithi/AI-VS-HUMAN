{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12906,
     "status": "ok",
     "timestamp": 1759594963836,
     "user": {
      "displayName": "Keerthi Seela",
      "userId": "08206637809797965557"
     },
     "user_tz": -330
    },
    "id": "_s6oSiYO9g1T",
    "outputId": "0bff04b3-d9af-4759-8b6d-6b16def9e374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "✅ Final Analysis:\n",
      "\n",
      "---\n",
      "### Probability Score\n",
      "The AI generation probability score is **0.5973**.\n",
      "\n",
      "---\n",
      "### Confidence\n",
      "The confidence level for this prediction is **Low**.\n",
      "\n",
      "---\n",
      "### Explanation\n",
      "The text exhibits a stilted and overly formal tone, with repetitive phrases like \"excellent and crucial point\" and \"absolutely right.\"  The language lacks natural variation and feels overly rehearsed, suggesting an AI's attempt to mimic human conversation.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch transformers openai groq\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from openai import OpenAI\n",
    "from google.colab import userdata\n",
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "try:\n",
    "    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
    "    os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "    GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve secrets. Please ensure they are set in your Colab environment. Error: {e}\")\n",
    "    HF_TOKEN = None\n",
    "    GROQ_API_KEY = None\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "def run_hf_detector(text, model_id=\"roberta-base-openai-detector\"):\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_id, token=HF_TOKEN).to(device)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "       \n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy()[0]\n",
    "\n",
    "    human_score = float(probs[0])\n",
    "    ai_score = float(probs[1])\n",
    "    label = \"AI-generated\" if ai_score > human_score else \"Human-generated\"\n",
    "    return {\"ai_score\": ai_score, \"human_score\": human_score, \"hf_label\": label}\n",
    "\n",
    "def calculate_perplexity(text):\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    max_length = model.config.n_positions\n",
    "    if encodings.input_ids.size(1) > max_length:\n",
    "        encodings.input_ids = encodings.input_ids[:, :max_length]\n",
    "        encodings.attention_mask = encodings.attention_mask[:, :max_length]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings, labels=encodings.input_ids)\n",
    "    loss = outputs.loss\n",
    "    perplexity = math.exp(loss.item())\n",
    "    label = \"AI-generated\" if perplexity < 60 else \"Human-generated\"\n",
    "    return {\"perplexity\": perplexity, \"perplexity_label\": label}\n",
    "\n",
    "def generate_explanation(text, ai_score, human_score):\n",
    "\n",
    "    if not GROQ_API_KEY:\n",
    "        return \"GROQ_API_KEY not found. Cannot generate explanation.\"\n",
    "\n",
    "    try:\n",
    "        groq_client = OpenAI(api_key=GROQ_API_KEY, base_url=\"https://api.groq.com/openai/v1\")\n",
    "        decision = \"AI-generated\" if ai_score > human_score else \"Human-generated\"\n",
    "\n",
    "        prompt = (\n",
    "            f\"You are an AI text analysis expert. Based on the following text, provide a concise, one or two-sentence explanation for why it was classified as '{decision}'. \"\n",
    "            \"Focus on the text's style, word choice, and structure. Do not mention the scores or probabilities in your explanation.\\n\\n\"\n",
    "            f\"Text: \\\"{text}\\\"\\n\\n\"\n",
    "            \"Explanation:\"\n",
    "        )\n",
    "\n",
    "        resp = groq_client.chat.completions.create(\n",
    "            model=\"gemma2-9b-it\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=150,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Could not generate explanation: {str(e)}\"\n",
    "\n",
    "def analyze_text(text):\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    try:\n",
    "        hf_out = run_hf_detector(text)\n",
    "        results.update(hf_out)\n",
    "        diff = abs(results[\"ai_score\"] - results[\"human_score\"])\n",
    "        if diff > 0.8:\n",
    "            results[\"confidence\"] = \"High\"\n",
    "        elif 0.3 <= diff <= 0.8:\n",
    "            results[\"confidence\"] = \"Medium\"\n",
    "        else:\n",
    "            results[\"confidence\"] = \"Low\"\n",
    "    except Exception as e:\n",
    "        print(f\"HF Detector error: {e}\")\n",
    "        results[\"hf_label\"] = \"Error\"\n",
    "        results[\"confidence\"] = \"Low\"\n",
    "\n",
    "    try:\n",
    "        results.update(calculate_perplexity(text))\n",
    "    except Exception as e:\n",
    "        print(f\"Perplexity error: {e}\")\n",
    "        results[\"perplexity_label\"] = \"Error\"\n",
    "\n",
    "    if \"Error\" not in [results.get(\"hf_label\"), results.get(\"perplexity_label\")]:\n",
    "        if results.get(\"hf_label\") != results.get(\"perplexity_label\"):\n",
    "            print(\"Notice: Detector and Perplexity models disagree. Confidence set to Low.\")\n",
    "            results[\"confidence\"] = \"Low\"\n",
    "\n",
    "    if \"ai_score\" in results:\n",
    "        explanation = generate_explanation(text, results[\"ai_score\"], results[\"human_score\"])\n",
    "        results[\"explanation\"] = explanation\n",
    "    else:\n",
    "        results[\"explanation\"] = \"Could not run primary detector to generate an explanation.\"\n",
    "\n",
    "    return results\n",
    "\n",
    "sample_text = \"That is an excellent and crucial point. You are absolutely right. My previous solution had a logical flaw: it detected AI-written text that was spoken aloud, not an AI-generated voice. To do what you require, we must analyze the audio itself for the electronic fingerprints of a synthetic voice, rather than analyzing the words being spoken. I have completely rewritten the audio portion of the script to do this correctly.\"\n",
    "\n",
    "final_output = analyze_text(sample_text)\n",
    "\n",
    "print(\"\\n✅ Final Analysis:\\n\")\n",
    "\n",
    "probability_score = final_output.get(\"ai_score\", 0.0) \n",
    "confidence_level = final_output.get(\"confidence\", \"N/A\")\n",
    "explanation = final_output.get(\"explanation\", \"No explanation available.\")\n",
    "\n",
    "print(\"---\")\n",
    "print(f\"### Probability Score\")\n",
    "print(f\"The AI generation probability score is **{probability_score:.4f}**.\")\n",
    "print(\"\\n---\")\n",
    "print(f\"### Confidence\")\n",
    "print(f\"The confidence level for this prediction is **{confidence_level}**.\")\n",
    "print(\"\\n---\")\n",
    "print(f\"### Explanation\")\n",
    "print(f\"{explanation}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMR2Taorysx2zz6bY38QUAa",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
